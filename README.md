# laterally_sparse_features
Neural networks are a powerful function approximation tool which has the ability to model any function with arbitrary precision. For any function as a black box, it is able to reconstruct the function given the target and the input data. However, there are problems where the target is at least partially unknown. In such cases it is impossible for a traditional neural network to compute the gradient of the system. This problem is evident in sparse autoencoder systems where lateral inhibitions are required. Lateral inhibitions are usually imposed with extra lateral connections among nodes in the immediate vicinity of the hidden layer. However, it is computationally expensive, and results in a very complex architecture which is not scalable to real world problems. Thus it is often necessary to achieve structural constraints without such complexity limitations. Similar situation arises in case of multi-label classification problems without a known target. In such cases only an evaluation is accessible to the network, which states whether the sample was correctly classified or not. In this problem, it is necessary to derive the gradient for this partial information to solve the problem. The proposed model solves such problems using policy gradient algorithms. This approach allows for imposing any arbitrary non-differentiable constraints on the neural network system by deriving the required gradient from a system of rewards. Furthermore a novel form of sparsity with lateral inhibitions over the entire hidden layer is proposed. It is shown that the proposed model is not only able to achieve a much tighter and sparser representation, it also achieves similar or better results than traditional forms of sparsity.

Paper code for: https://ieeexplore.ieee.org/abstract/document/7966360
